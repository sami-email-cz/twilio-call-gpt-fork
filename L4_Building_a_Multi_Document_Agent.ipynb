{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sami-email-cz/twilio-call-gpt-fork/blob/main/L4_Building_a_Multi_Document_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b523e0a",
      "metadata": {
        "id": "0b523e0a"
      },
      "source": [
        "# Lesson 4: Building a Multi-Document Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a323703",
      "metadata": {
        "id": "1a323703"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install llama_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sntSf1EoWw5",
        "outputId": "841f175e-e98e-4333-fe12-766155cbc647"
      },
      "id": "8sntSf1EoWw5",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_index\n",
            "  Downloading llama_index-0.10.37-py3-none-any.whl (6.8 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama_index)\n",
            "  Downloading llama_index_agent_openai-0.2.5-py3-none-any.whl (13 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.35 (from llama_index)\n",
            "  Downloading llama_index_core-0.10.37-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama_index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.9-py3-none-any.whl (6.0 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama_index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama_index)\n",
            "  Downloading llama_index_llms_openai-0.1.19-py3-none-any.whl (11 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama_index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl (5.8 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama_index)\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama_index)\n",
            "  Downloading llama_index_readers_file-0.1.22-py3-none-any.whl (36 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.3.0,>=0.1.4->llama_index)\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (3.9.5)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.35->llama_index)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.35->llama_index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.35->llama_index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.35->llama_index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpath-ng<2.0.0,>=1.6.0 (from llama-index-core<0.11.0,>=0.10.35->llama_index)\n",
            "  Downloading jsonpath_ng-1.6.1-py3-none-any.whl (29 kB)\n",
            "Collecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core<0.11.0,>=0.10.35->llama_index)\n",
            "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (2.31.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (3.7.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (8.3.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.35->llama_index)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (4.11.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.35->llama_index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama_index) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index)\n",
            "  Downloading llama_parse-0.4.3-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama_index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama_index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama_index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
            "Collecting ply (from jsonpath-ng<2.0.0,>=1.6.0->llama-index-core<0.11.0,>=0.10.35->llama_index)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama_index) (2.7.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama_index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama_index) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.35->llama_index)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama_index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.35->llama_index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama_index) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.35->llama_index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.35->llama_index) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (6.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (3.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.35->llama_index) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.35->llama_index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.35->llama_index)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama_index) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama_index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.2.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama_index) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama_index) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama_index) (1.1.1)\n",
            "Installing collected packages: striprtf, ply, dirtyjson, pypdf, mypy-extensions, marshmallow, jsonpath-ng, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
            "Successfully installed dataclasses-json-0.6.6 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpath-ng-1.6.1 llama-index-agent-openai-0.2.5 llama-index-cli-0.1.12 llama-index-core-0.10.37 llama-index-embeddings-openai-0.1.9 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.19 llama-index-multi-modal-llms-openai-0.1.6 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.22 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.3 llama_index-0.10.37 llamaindex-py-client-0.1.19 marshmallow-3.21.2 mypy-extensions-1.0.0 openai-1.30.1 ply-3.11 pypdf-4.2.0 striprtf-0.0.26 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "path_m = \"/content/\"\n",
        "\n",
        "sys.path.insert(0, path_m)"
      ],
      "metadata": {
        "id": "AdsZC1Sen0hi"
      },
      "id": "AdsZC1Sen0hi",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f"
      },
      "outputs": [],
      "source": [
        "#from helper import get_openai_api_key\n",
        "#OPENAI_API_KEY = get_openai_api_key()\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "import openai\n",
        "openai.api_key = OPENAI_API_KEY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3221a474-5817-4db2-af46-e029042a75a5",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "3221a474-5817-4db2-af46-e029042a75a5"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20adaa26",
      "metadata": {
        "id": "20adaa26"
      },
      "source": [
        "## 1. Setup an agent over 3 papers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48b71ff6",
      "metadata": {
        "id": "48b71ff6"
      },
      "source": [
        "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
      "metadata": {
        "height": 200,
        "tags": [],
        "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"/content/metagpt.pdf\",\n",
        "    \"/content/longlora.pdf\",\n",
        "    \"/content/selfrag.pdf\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
      "metadata": {
        "height": 149,
        "tags": [],
        "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c54c995-a995-4fc0-ed19-67a5527c2034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting tools for paper: /content/metagpt.pdf\n",
            "Getting tools for paper: /content/longlora.pdf\n",
            "Getting tools for paper: /content/selfrag.pdf\n"
          ]
        }
      ],
      "source": [
        "from utils import get_doc_tools\n",
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07"
      },
      "outputs": [],
      "source": [
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bff58c52",
      "metadata": {
        "height": 64,
        "id": "bff58c52"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2f2c6a9f",
      "metadata": {
        "height": 30,
        "id": "2f2c6a9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc7e41d-b9ba-4d07-cef1-fbcf211d4689"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "len(initial_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a124a438-5609-402e-8642-69d1088cb9ad",
      "metadata": {
        "height": 166,
        "tags": [],
        "id": "a124a438-5609-402e-8642-69d1088cb9ad"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    initial_tools,\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a68a82-f97b-493c-bff1-e2142322c044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
            "=== Function Output ===\n",
            "PG19\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
            "=== Function Output ===\n",
            "The evaluation results include reporting perplexity for models and baselines on proof-pile (Azerbayev et al., 2022) and PG19 datasets. The models achieve better perplexity with longer context sizes, indicating the effectiveness of the fine-tuning method. The perplexity decreases as the context size increases, with improvements observed when increasing the context window size. Additionally, the maximum context length that can be fine-tuned on a single 8 × A100 machine is extended for different models, showing promising results on extremely large settings. Some perplexity degradation is noted on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset used in the experiments is the PG19 test split.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation results\"}\n",
            "=== Function Output ===\n",
            "The evaluation results indicate that the models achieve improved perplexity with longer context sizes, demonstrating the effectiveness of the efficient fine-tuning method. The models show better performance as the context size increases, highlighting the success of extending the context window. Additionally, the models are fine-tuned to efficiently handle extremely large context lengths with minimal accuracy compromise. The passkey retrieval accuracy remains reasonable up to a certain context length, and the model can manage longer documents by extending the position interpolation. Furthermore, the attention patterns during fine-tuning are compared, with S2-Attn showing effectiveness in approximating standard self-attention patterns. The efficiency analyses conducted on models like Llama2 and LongLoRA showcase the performance improvements achieved with different context lengths and fine-tuning methods, along with efficiency gains in training speed and GPU memory cost attributed to S2-Attn.\n",
            "=== LLM Response ===\n",
            "The evaluation dataset used in LongLoRA is the PG19 test split. The evaluation results show that the models achieve better perplexity with longer context sizes, demonstrating the effectiveness of the fine-tuning method. The models perform well as the context size increases, with improvements observed when increasing the context window size. Additionally, the models can handle extremely large context lengths efficiently with minimal accuracy compromise. The passkey retrieval accuracy remains reasonable for longer documents, and the attention patterns during fine-tuning are compared, with S2-Attn showing effectiveness in approximating standard self-attention patterns. The efficiency analyses on models like Llama2 and LongLoRA highlight performance improvements with different context lengths and fine-tuning methods, along with efficiency gains in training speed and GPU memory cost attributed to S2-Attn.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
        "    \"and then tell me about the evaluation results\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ace340b1-761f-4058-be41-68cf131541e4",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "ace340b1-761f-4058-be41-68cf131541e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9722c47e-60e5-49aa-8ac2-9099ad568bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
            "=== Function Output ===\n",
            "Self-RAG is a framework designed to enhance the quality and factuality of large language models (LLMs) by incorporating retrieval on demand and self-reflection. It involves training a Critic LM and a Generator LM to evaluate text based on various criteria such as relevance, supportiveness, and usefulness. The system utilizes reflection tokens to assess if the generated output is supported by evidence and incorporates human evaluations to ensure the accuracy and informativeness of responses. Through careful curation of training data and leveraging reflection tokens, Self-RAG aims to tailor LM behaviors at test time, leading to significant performance enhancements compared to existing LLMs and traditional retrieval-augmented generation methods.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
            "=== Function Output ===\n",
            "LongLoRA is an efficient fine-tuning framework that extends the context sizes of pre-trained large language models with minimal accuracy compromise. It introduces shifted sparse attention (S2-Attn) to approximate the standard self-attention pattern during training, allowing for significantly larger context lengths while reducing GPU memory cost and training time compared to standard full fine-tuning. Additionally, LongLoRA retains the original attention architecture during inference, making it compatible with existing optimization and infrastructure.\n",
            "=== LLM Response ===\n",
            "Self-RAG is a framework designed to enhance the quality and factuality of large language models (LLMs) by incorporating retrieval on demand and self-reflection. It involves training a Critic LM and a Generator LM to evaluate text based on various criteria such as relevance, supportiveness, and usefulness. The system utilizes reflection tokens to assess if the generated output is supported by evidence and incorporates human evaluations to ensure the accuracy and informativeness of responses. Through careful curation of training data and leveraging reflection tokens, Self-RAG aims to tailor LM behaviors at test time, leading to significant performance enhancements compared to existing LLMs and traditional retrieval-augmented generation methods.\n",
            "\n",
            "LongLoRA is an efficient fine-tuning framework that extends the context sizes of pre-trained large language models with minimal accuracy compromise. It introduces shifted sparse attention (S2-Attn) to approximate the standard self-attention pattern during training, allowing for significantly larger context lengths while reducing GPU memory cost and training time compared to standard full fine-tuning. Additionally, LongLoRA retains the original attention architecture during inference, making it compatible with existing optimization and infrastructure.\n",
            "assistant: Self-RAG is a framework designed to enhance the quality and factuality of large language models (LLMs) by incorporating retrieval on demand and self-reflection. It involves training a Critic LM and a Generator LM to evaluate text based on various criteria such as relevance, supportiveness, and usefulness. The system utilizes reflection tokens to assess if the generated output is supported by evidence and incorporates human evaluations to ensure the accuracy and informativeness of responses. Through careful curation of training data and leveraging reflection tokens, Self-RAG aims to tailor LM behaviors at test time, leading to significant performance enhancements compared to existing LLMs and traditional retrieval-augmented generation methods.\n",
            "\n",
            "LongLoRA is an efficient fine-tuning framework that extends the context sizes of pre-trained large language models with minimal accuracy compromise. It introduces shifted sparse attention (S2-Attn) to approximate the standard self-attention pattern during training, allowing for significantly larger context lengths while reducing GPU memory cost and training time compared to standard full fine-tuning. Additionally, LongLoRA retains the original attention architecture during inference, making it compatible with existing optimization and infrastructure.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eede70c",
      "metadata": {
        "id": "7eede70c"
      },
      "source": [
        "## 2. Setup an agent over 11 papers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18771e69",
      "metadata": {
        "id": "18771e69"
      },
      "source": [
        "### Download 11 ICLR papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
      "metadata": {
        "height": 472,
        "tags": [],
        "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
        "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
        "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
        "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
        "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
        "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
        "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"loftq.pdf\",\n",
        "    \"swebench.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "    \"zipformer.pdf\",\n",
        "    \"values.pdf\",\n",
        "    \"finetune_fair_diffusion.pdf\",\n",
        "    \"knowledge_card.pdf\",\n",
        "    \"metra.pdf\",\n",
        "    \"vr_mcl.pdf\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b77426cb",
      "metadata": {
        "tags": [],
        "id": "b77426cb"
      },
      "source": [
        "To download these papers, below is the needed code:\n",
        "\n",
        "\n",
        "    #for url, paper in zip(urls, papers):\n",
        "         #!wget \"{url}\" -O \"{paper}\"\n",
        "    \n",
        "    \n",
        "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
      "metadata": {
        "height": 149,
        "tags": [],
        "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "808304b4-9165-4b23-e3a4-852a3cc962ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting tools for paper: metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n",
            "Getting tools for paper: loftq.pdf\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "File loftq.pdf does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-869505759b61>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpaper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpapers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Getting tools for paper: {paper}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvector_tool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_tool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_doc_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpaper_to_tools_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvector_tool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_tool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mget_doc_tools\u001b[0;34m(file_path, name)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# load documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleDirectoryReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0msplitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_nodes_from_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/readers/file/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dir, input_files, exclude, exclude_hidden, errors, recursive, encoding, filename_as_id, required_exts, file_extractor, num_files_limit, file_metadata, raise_on_error, fs)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File {path} does not exist.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File loftq.pdf does not exist."
          ]
        }
      ],
      "source": [
        "from utils import get_doc_tools\n",
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e35d52c",
      "metadata": {
        "id": "4e35d52c"
      },
      "source": [
        "### Extend the Agent with Tool Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "20154923-873e-4941-9a3a-4926ab5f9b8c"
      },
      "outputs": [],
      "source": [
        "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
      "metadata": {
        "height": 149,
        "tags": [],
        "id": "671582f9-70d7-4a8f-b813-58b2a068ca72"
      },
      "outputs": [],
      "source": [
        "# define an \"object\" index and retriever over these tools\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.objects import ObjectIndex\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "c3929882-e9dc-46ca-b495-53e3ed60340e"
      },
      "outputs": [],
      "source": [
        "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03"
      },
      "outputs": [],
      "source": [
        "tools = obj_retriever.retrieve(\n",
        "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5"
      },
      "outputs": [],
      "source": [
        "tools[2].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
      "metadata": {
        "height": 251,
        "tags": [],
        "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_retriever,\n",
        "    llm=llm,\n",
        "    system_prompt=\"\"\" \\\n",
        "You are an agent designed to answer queries over a set of given papers.\n",
        "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
        "\n",
        "\"\"\",\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "a250cf1a-e011-4994-bcca-4e0294f20864"
      },
      "outputs": [],
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used \"\n",
        "    \"in MetaGPT and compare it against SWE-Bench\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d"
      },
      "outputs": [],
      "source": [
        "response = agent.query(\n",
        "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
        "    \"Analyze the approach in each paper first. \"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}